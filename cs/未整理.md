#ml
		Neural Network
		形式ニューロン
		パーセプトロン
		順伝播型ニューラルネットワーク	入力xを入れて、出力yを得る。Neural Networkの本質は、「損失関数の最小化」である。出力yと、正しい出力y_nのズレを小さくなるようにしたい
		活性	各ノードからの重み付き和
		出力	活性にバイアスを加え、さらに活性化関数で変換したもの
		誤差関数 E(ω)	目標と現在値のズレを測るための関数
		学習	本質的には、誤差関数の最小化のこと

	深層学習の正規化
		機械学習の目的
		A	二つの要素をできるだけ近づけること
				モデルが導き出す予測
				データ生成分布のふるまい
		B	汎化誤差を最小化すること
		しかし、現実的には
			データ生成分布を知ることは不可能
			汎化誤差を正確に計算することも不可能
		ではどうするか？
			訓練データから経験分布 empirical distributionを得る
				訓練データDの各データの出現頻度を記したもの
			経験分布をもとに汎化誤差を見積もる
		しかし、汎化誤差と訓練誤差は実際には別々であるので、問題が生じる
			これが過学習である
				過学習は、テスト誤差 test errorで測ることができる
			更新を続けていくと、訓練誤差はどんどん小さくなる
			更新を続けていくと、テスト誤差も小さくなるが、ある点で最小点をとったあとテスト誤差はむしろ拡大してしまう
		では、過学習をどのようにして防ぐか？
			データの数を増やす
				現実的には難しいが
	正規化
	誤差逆伝播法 Backprop (Backpropagation Method)
		目的
			勾配降下法により誤差関数の谷を下っていきたいのだが、実際のneural networkでは誤差関数はそれほど自明ではない
			そこで、勾配を高速に計算するための方法が誤差逆伝播法である
			微分のチェインルールを利用している
		誤差逆伝播法に至るまで
			パーセプトロンの学習則
			Wildrow-Hoff Rule / Delta Rule / LMS (Least Mean Square) Rule
		誤差逆伝播法の問題点
			勾配消失問題 vanishing gradient problem / 勾配爆発問題　exploding gradient problem
			Solution
				Pretraining 事前学習
				ReLU function
	Autoencoder 自己符号化器
		What for?
			Reduce the dimension of the data
		What's this?
			Autoencoder is an example of unsupervised learning
			Uses neural network
		Review dimension reduction methods w/o autoencoder:
			主成分分析　PCA: primary component analysis
	Deep Reinforcement Learning
		What for?
			Leaning w/o data labelling (unsupervised)
		Agent vs Env
			Agent
				Gives A (action) to the environment
			Environment
				Gives R (reward) and S (state) to the agent
		"Episode"
			flow from the start of the game and end of it
book

misc
	反復再重み付け最小二乗法　IRLS
	deepfake
	Follow the Regularized Leader" (FTRL) algorithm
		原論文
			2013, Ad Click Prediction: a View from the Trenches, by google
		GoogleがCTR（広告のクリック率）を機械学習した
		重みと学習計数の計算方法
			普通はOGD(オンライン確率降下法)
			この論文ではFTRL-Proximalアルゴリズム
	モデル
		識別モデル
		生成モデル
	VQA: visual question answering
	Hammersley-Clifford定理
	因子分析
	閉じた形の解　
		無限の形がなく、有限個の項で表された式の解
	閉形式
		加減乗除や初等関数の合成関数による解の表し方を、閉形式(closed-form)という。
		微分形式でいう閉形式とは異なる。 ただし、初等関数としてどれだけの関数を含めるかは、明確に定められていないので、閉形式の定義は曖昧である。しかし、この曖昧さは実用上はほとんど問題にならない。
	ベッセルフィルタ
	カットオフ周波数
	block chain
	PMD
		polarization mode dispersion
		偏波モード分散
	ランダムウォーク
	カーネル法 bishop_chap6
		双対表現
		RBFネットワーク：動径基底関数
		ガウス過程
	サンプリング法: bishop_chap10
		棄却サンプリング
		適応的棄却サンプリング
		重点サンプリング
		SIR
		マルコフ連鎖モンテカルロ法
		ギブスサンプリング
		ハイブリッドモンテカルロアルゴリズム
	連続潜在変数 bishop_chap12
		主成分分析
		確率的主成分分析
		カーネル主成分分析
	系列データ: bishop_chap13
		マルコフモデル
		線形動的システム: LDS
	主成分分析
	feature pyramid network
			ホップフィールドネットワーク
			用途
				次元圧縮
	グラフィカルモデル
		ベイジアンネットワーク
		マルコフ確率場
			説明
				画素ベースの手法が多い
			用途
				画像修復
				領域分割
				画像合成
		MCMC: Markov chain Monte Carlo methods マルコフ連鎖モンテカルロ法
			用途
				確率分布のサンプリングを行うアルゴリズムの総称
			仮定
				Detailed Balance 詳細釣り合い条件
				Ergodicity エルゴート性
			sub
				Metropolis法
					Metropolis Light Transportとかいう、CGのレンダリングアルゴリズムに応用できるらしい
				Metropolis-Hastings法
				Gibbsサンプリング
				ハミルトニアンモンテカルロ法
		劣モジュラ関数	センサ配置問題などに応用される
	バンディットタスク
	意思決定
		A/B test
		Bandit Algorithm
因果関係
	ベイジアンネットワーク
	統計的因果探索
		LINGAM
		GARCH model	modelling the fluctuation of variance (分散)
Deep Learning
	利点
		 特徴量を自分で持ってくることができる
		 End to Endでタスクを扱える
		End-To-End learning
	CNN: Convolution Neural Network
		レイヤ構成
			例	線形非ガウス状態空間モデル（動的一般化線形モデル）
				->(Conv-ReLU-Pooling)->(Conv-ReLU-Pooling)->(Conv-ReLU)->(Affine-ReLU)->(Affine-Softmax)
			従来のNNの例（参考）
				->((Affine-ReLU)->((Affine-ReLU)->((Affine-ReLU)->(Affine-Softmax)
				非線形非ガウス状態空間モデル（一般化状態空間モデル, or simply 状態空間モデル in some context）
			畳み込み層 Conv
				入力特徴マップ
			estimation method classification
				出力特徴マップ
					層からの出力データのこと
		曲線フィッティング
			prophet法
					３次元の場合：各ピクセルがチャネルを持つ（RGBとか）
						２次元の場合をブロックで考える
							元のデータ
			機械学習法					channel-height-width（C、H、W）
							filter (チャネル数は同じ)
								(C, FH, FW)
				疑問
					フィルタの形状は必ず正方形か？
			プーリング層
				性質
					縦・横方向の空間を小さくする
					学習パラメータがない
						poolingのwindow size, stride は調整すべきパラメータだと思えるが???
					poolingによってchannel数が変化しない。
						チャネル毎に独立して演算するため
					微小な画像のズレに対してロバスト
				種類
					Maxプーリング
					Averageプーリング
		性質
			全結合ではないため、画像をそのまま３次元情報として利用できる
				全結合の問題点：
					データの形状が無視され、データ同士のつながりの情報が捨てられてしまう
						画像を全結合のネットワークに投入するとき、１次元に変換するので
			層が深くなるほど精度がます
				理論的説明は不十分だが、一応の理由
					層が深いほど、パラメータの数が減る
					層が深いほど、学習データが少なくて済む
		分類
			CNN
			R-CNN: regions with CNN features
			fast R-CNN
			faster R-CNN, 2015
				RPN:region proposal network を作る
					RPNは領域提案ネットワークであり、特徴マップから物体領域候補を推定する
			mask R-CNN
			YOLO: you only look once
	architecture
		LeNet	LeCun, 1998
		AlexNet	ILSVRC 2012, Hinton
			ディープラーニングブームの火付け役
		ZFNet	ILSVRC 2013 winner
		GoogLeNet	Google, ILSVRC 2014 winner
		VGG-Net	2014
			特徴
				->(Conv-ReLU-Conv-ReLU-Pooling)の繰り返し->(Affine-ReLU-Dropout)->(Affine-Dropout-Softmax)->
				層が深くなるほど、チャネル数が指数関数的に増大していく
				filter size:
					3x3
				activation func:
					ReLU
				use Dropout
				use Adam
				use Heの初期値 as a initial weight
			実装
				VGG-16
					->
					-( (Conv2D+ReLU)^2 + MaxPooling)^2
					-( (Conv2D+ReLU)^3 + MaxPooling)^2
					-( (FullyConnected+ReLU)^3 + Softmax)
					->
		ResNet
			by microsoft, including Kaiming He
			ILSVRC2015
	FCN: Fully Convolutional Network
Reinforcement Learning
	category
		普通の強化学習

Toy Problem
	生成
		正規分布の乱数
		sinカーブ　+ 正規分布ノイズ
	logic gate 論理ゲート（パーセプトロンのテストなど）
		OR
		AND
		XOR
	images
		MNIST handwriting
			70000 data
				60000 for training
				10000 for test
			28px 28px
			0 to 9 number
		Iris
		Wine
		Housing
			UCI Machine Learning Repository
		Breast Cancer
		Titanic
		Cifar 10
			60000 image
				10 different categories and 6,000 images per category.
			only 32 by 32 pixels.
		Caltech 101
		Caltech 256
		Imagenet
		bAbI タスク
			 20種の問題形式で構成されたトイタスクのデータセット
			 自然言語処理用
		Open Images Dataset
		Kaggle Dataset
	API to get data
		Twitter
		Facebook
		Flickr
		Instagram
		Google Custom Search API
		Bing Search API
計算グラフの生成タイミング
	define by run
		データを流したときに計算グラフを生成
		利点
			動的なネットワークを構築することが出来る (Ex. mini-batch毎に処理が違っても良い)
			モデルのエラーのデバッグが行いやすい
		欠点
			グラフを構成するのが動的なのでオーバーヘッドが発生する
			計算の最適化が行いにくい
		実例
			chainer
			pyTorch
			TensorflowのEager
	define and run
		あらかじめ計算グラフを生成

機械学習の種類
	1. 教示データ有無による分類
		supervised
			subcategory
				線形回帰 Linear Regression, by Linear Classifier 線形分類器
					単純パーセプトロン
					adaline
					ロジスティック回帰　（DLTK３章）
						用途
							線形にしか使えない
							出力値
								パーセプトロンでは、０か１かの離散値
								ロジスティック回帰では０から１までの確率
									p=0.5より大きいか小さいかで０か１か二値化することは当然可能だが。
						種類
							普通のロジスティック回帰
								２クラス分類のみ
							多クラスロジスティック回帰
							ベイズロジスティック回帰
						重みの更新方法
							バッチ勾配降下法
								一番原始的
								全てのノードの値を用いて更新
							ミニバッチ勾配降下法（広義の確率的勾配降下法）
								M=50ー５００くらいのデータに分けて学習する
								確率的勾配降下法（狭義）SGD
									オンライン学習
									M=１の場合のミニバッチ勾配降下法に相当する
									データを１つずつランダムに選んで更新
						学習モデルの評価
							ROC曲線
					フィッシャーの線形判別分析
					単純ベイズ分類器(naive Bayes classifier)
					Linear SVM:
					正則化を用いる線形回帰 （Raschka 10)
						Ridge Regression
							L2正則化
						Lasso Regression
							L1正則化
						ElasticNet
							L1正則化とL2正則化
				Neural Network
					多層パーセプトロン
						非線形も分類可能
				Polynomical Regression 多項式回帰 （Raschka 10)
				k近傍法: k -nearest classifier, KNN,
					lazy learner の一種
				Decision Trees 決定木回帰
					利点
						特徴量を変換する必要が一切ない
				Random Forest
					利点
						分類性能が高い
						スケーラビリティに優れる
					手順
						1
							大きさnのランダムなブートストラップ標本を復元抽出
						2
							ブートストラップ標本の各ノードから決定木を成長させる
								2.1
									d個の特徴量をランダムに非復元抽出する
								2.2
									目的関数に従って最適な分割となる特徴量を使い、ノードを分割する
						3
							1-2をk回繰り返す
						4
							決定木毎の予測で多数決
							　
				サポートベクター回帰
				射影追跡回帰
				多変量適応的回帰スプライン
				二次判別分析
				ベイジアンネットワーク
				サポートベクターマシン
				隠れマルコフモデル
				Bagging
				Boosting
				ベイズ分類器
					最尤法でパラメータ決定する
		Unsupervised
			種類
				clustering algorithm (Raschka chap11)
					サンプルークラスタの対応関係による分類
						hard clustering
							一つのサンプルは、一つのクラスタに属する
							e.g.
								k-means clustering
						soft clustering, aka fuzzy clustering
							一つのサンプルが、複数のクラスタに属する
							e.g.
								FCM: Fuzzy c-means, aka Soft k-means, aka Fuzzy k-means
					subcategory
						k-means clustering: k平均法
							k近傍法と混同しない
						k-means++ clustering
						HCA: hieararchical clustering analysis
						Expectation Maximization (EM)
							最尤推定を利用
								使用する確率分布の種類
									ベルヌーイ分布
									混合分布
						DBSCAN
					クラスタリングの評価方法
						Elbow Method
						Silhouette Analysis
				Association Rule Learning 共起分析 , aka アフィニティー分析 (ITML-chapter)
					Apriori
					Eclat
				Visualization & Dimension Reduction 次元削減
					PCA：主成分分析
					Kernel PCA
					ICA: 独立成分分析
					多様体学習
						多様体
							局所的に見たら線形空間で表すことのできるような空間
							スイスロールの渦巻きは３次元だが、本質的には２次元（伸ばせるので）
								これを、３次元空間に埋め込まれた２次元多様体という
							スイスロールの内側の線と外側の線が近接していても、実際には渦巻き一周分の距離がある
								多様体を使うと、このような「見かけの距離」ではなく「実質的に意味がある距離」で問題を処理することができる
						subcategory
							LLE: Locally Linear Embedding
							modified LLE
							t-SNE: t-Distributed Stochastic Neigbor Embdding
				未分類
					deep belief network
					deep boltzmann machine
					DCGAN
	情報収集と学習のタイミング
		Batch Learning	データが追加されたら、既存データを含めて全てモデル作成をやり直す
		Online Learning	新しいデータが到達するたびにその都度、あたらしいデータのみを使って学習する
		Follow the Leader (FTL) algorithm
		Regret Minimization (Regret Analysis)
	1. 結果がいつも同じかどうかによる分類
		確率論的
		決定論的
	4. 一般化手法による分類
		Model-Based
			モデルを構築する。モデル構築後は既知の個々の事例そのものは顧みない
		Instance-Based
			既知の事例からのSimilarityにもとづいて一般化する
	5. NNによる分類
		NNを使わないもの
		NNを使うもの
			層の数
				単層
					パーセプトロン
						性質
							確率的勾配降下法を利用
						欠点
							XORが解けない
								中間層の結合荷重が変わらないから
				多層
			ノードの結合
				全結合
				スパース結合

	Configuring ML
		出力をどの指標で評価するのか？
			交差エントロピー誤差関数
		その指標をどのアルゴリズムで最適化し、重みを更新するのか？
			勾配降下法
		どの活性化関数を採用するのか？
		ハイパーパラメータをいくつにするのか？
			学習率
		どのように初期化するか？
			重み
発想による手法の分類
	高速化したい
		計算量を減らす
			次元削減
			局所的受容野(local receptive field)
			重み共有(weight sharing)
		より良いアルゴリズムを採用する
		より良いマシンを採用する
		分散学習
		演算精度のビット削減
			Deep Learningの場合
				16ビットの半精度浮動小数点数までは減らして大丈夫だといわれている
	学習精度を向上したい
		少ないデータでも効率を上げる
			ブートストラップ法
				少ないデータから多くのパターンを生成
			Transfer Learning　転移学習
				ある領域で学習させたモデルを、別の領域に適応させる技術
				学習済みモデルの重みを変更せず、そのまま使う
				第一段
					入力画像から、特徴量(ボトルネック特徴量)を抽出する
						方法
							学習済みモデルから出力層を取っ払う
							入力画像をモデルにつっこみ、出てきた特徴量を使う
				第二段
					ボトルネック特徴量を用いて、クラス分類をする
						出てきた特徴量に対して学習・予測を行う
				用途
					広くデータが手に入る領域で学習させたモデルを少ないデータしかない領域に適応させる
					シミュレーター環境で学習させたモデルを現実に適応させるとか
				利点
					少ないデータしか得られない領域でのモデル構築
			Fine Tuning
				ある領域で学習させたモデルを、別の領域に適応させる技術
				学習済みモデルの重みを一部変更して使用
			画像を自分で作る
				元の画像から水増し
					keras.preprocessing.image.ImageDataGeneratorなどを使う
				シミュレータ・物理エンジン・CGを使う
					SYNTHIA dataset
						自動運転用のCG画像
		学習精度の評価方法
			目的関数の種類
				損失関数 (Loss Function) aka 誤差関数
				コスト関数
					損失関数に正則化項を足したもの
				しかし、損失関数とコスト関数は区別されないこともある
			目的関数の最適化手法の選択
				平均自乗誤差 MSE: Mean Squared Error
				交差エントロピー Cross- Entropy
				L1: Least absolute deviations　最小絶対偏差
				L2: Least square errors 最小二乗誤差
				Maximum Likelihood 最大尤度
				誤差逆伝播法
					損失関数の微分を求める手法
				Cross-Entropy
				Hinge
				Huber
				Kullback-Leibler
		より良いモデルを選択する（Raschka Chap6)
			指標
				指標計算の前提知識：混同行列 confusion matrix
					分類ができた・できなかった数をカウントした表
					区分
						 TP
						 FN
						 FP
						 TN
				ACC: 正解率 accuracy
					TP+TN / FP +FN + TP +TN
					 ERR 誤分類率
						1 - ACC
				PRE: 適合率（精度） precision,
					 TP/(TP+FP)
						見つけたもののうち、それが本当に欲しいものである割合。
				REC: 再現率 recall
				F値(F-value)
				網羅率(coverage)
			汎化性能の指標
				ｋ分割交差検証 k-fold cross validation
				ホールド・アウト法 hold out
				学習曲線   learning curveを書く
				検証曲線 validation curveを書く
				Leave-one-out-法
				Out-of-Bag法
				汎化性能の理論については、VC理論が関係しているらしい
					計算論的学習理論
						極限における同定
						PAC学習
						VC理論
				まず、指標で状況を把握する
					learning curveの形状をみる
					variance
						overfitのとき高くなる
					bias
						underfitのとき高くなる
			Leakageを防ぐ
				データセットの行番号などは、leakageの原因になる
				leakageがあると、accuracyが100%近くなったりする異常事態が起きる
			過学習を防ぐ
				データ数を増やす
				データの質の向上
					Sampling Biasの少ないデータを取得する
				バッチ学習
					擬似的なアンサンブル学習
				荷重減衰 weight decayを強める
					L1正則化
					L2正則化
					L∞正則化
				dropout
					ニューロンをランダムに除外する
					原理
						実質的に、複数のモデルを生成・学習していることになるため→???
				アンサンブル学習
					dropoutとは違い、実際に複数のモデルを使って学習する
					分類
						多数決分類器
						バギング
						Boosting
							AdaBoost
				early stopping
			Underfittingの防止
				パラメータを増やす
				より優れた特徴量を選択する
		勾配消失への対応
			隠れ層の活性化関数にSigmoidではなく、ReLU系を使う
	計算精度
		ゼロに近い値がでることによるミス防止
			logをとる
	データの前処理
		欠測データへの対処
			欠陥がある特徴量orサンプルを全削除
			補完する
		スケーリングの影響を排除する
			正規化
			標準化
		Batch Normalization
			ミニバッチごとに正規化する
		次元削減
			特徴選択
				有益な特徴量の選択
				ランダムフォレストによる特徴選択
			特徴抽出
	重みの効率的な初期化
		全て０で初期化
			欠点
				誤差逆伝播の際に勾配の値も同じになってしまう
		０に近い乱数で初期化
		正規分布もしくは一様分布による初期化
			LeCun, 1988
			Glorot and Bengio, 2010
		Xavierの初期値
			1/sqrt(N)の標準偏差を持つガウス分布で初期化
			活性化関数が線形である場合に用いる
				tanhやsigmoidは中央付近で線形だとみなせる
		Heの初期値
			平方根を使ったっぽい初期化
			Kaiming He, 2015
				ReLUを用いる場合についてのみ
	学習率を最適化する：手法の候補
		純粋なSGD
		momentum
			学習率の値を直接は変えないが、調整項によって実現
			普通のmomentum
			Nesterov Momentum
				どの方向に向かうべきかを指定
		Adagrad
			学習率自体を変化
			学習率の減衰 learning rate decay
		Adadelta
		RMSprop
		Adam
			momentumとAdagradのあわせ技
	活性化関数の選択
		そもそも活性化関数になりやすい関数はどうやって決められたのか
			0から1に正規化されるものがいい
			微分が単純な式になるものがよい
				勾配を計算するため
		ステップ関数
		Sigmoid関数
			利点
				出力が0-1に正規化できる
				微分した形をSigmoid関数自身で表せる
				値域は0から1
			用途
				隠れ層
		恒等関数
		ReLU関数
			オリジナルのReLU
				利点
					導関数はxがどれだけ大きくなっても1を返すため、勾配が消失しない
					式が単純なので、計算量が減る
				欠点
					x < 0のときは勾配が0なので、ニューロンが一度不活性になるとずっと不活性のままになりえる→???
			Leaky ReLU: LReLU
				性質
					ReLUの改良版
					x <0でもわずかに傾きαをつける
				欠点
					効果があるときとないときがある
			Parametric ReLU: PReLU
				LReLUの改良版
				性質
					x<0での傾きを固定せず学習する
			Randomized ReLU
				実質的に複数モデルを使って学習するようなものなので、アンサンブル学習に似た側面がある
			Exponential Linear Units: ELU
		softmax関数
			性質
				シグモイドと違い、入力値が必ずベクトルになる
			利点
				出力が0-1に正規化できる
			用途
				出力層
		tanh関数
			性質
				Sigmoidに似ているが、勾配消失問題が起こりにくい
					理由
						Sigmoidの導関数の最大値が0.25に対し、tanhの導関数の最大値は1であるため→???
				値域は-1から1
			用途
				隠れ層
